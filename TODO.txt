# TODO workplan

## Good Data

Currently `overseas` and `redo` scrape data is good.
Older scraped data may be okay now that overseas comanies was scraped again recently.
This why it is important to rebuild the database again and prepare for continuous scraping to update the database at any time.

- [x] Handle upgrading entity stub to a fully detailed entity
  This can be done with an upsert at the beginning of the script that fills the database.
  So later scraping sessions may perhaps locate an entity then replace a stub with complete scraped details.
  Also, overtime new scraping sessions will replace old versions of known entities.

## BAD Data

- [ ] Scrape local companies again (in progress; with failures)

Appears early scrapes have too much bad data.
I can scrape again and have all good results.
For now I should focus on tools to help process and prepare the database then tools to get data into format for presentation.

- [ ] write to DB last_seen value to know how long since a company was scraped.
  This identifies failed scrapes and old data; allows us to make less costly scraping
- [ ] read list view company names for purpose of skipping listings and skipping pages

## 2024 08 03 WIP

vfsc spider    >>> scraper -> JSONL
process.py     >>> JSONL -> distinct -> pkl
test-sqlite.py >>> pkl -> db

Need periodic scrape
 - target company
 - specific query

Update lastseen

Setup DB
 - In pipeline to spider
 - Store history & current state of world

Track usage and queries
 - separate web db?
 - structure logfiles for parsing later?

Cache
 - NetworkX
 - Pages

---

follow insert_company_data
- [x] 0. check hash of scraped data against history
- [x] 1. insert or replace company > get ID
         - remove all old directores, shareholders ?
- [ ] 2. replace all directors, shareholders, etc.
- [x] 3. record history change
